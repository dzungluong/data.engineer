{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"e038fc6c-88f0-4d75-bef8-55d73e3ba2c5","known_lakehouses":[{"id":"e038fc6c-88f0-4d75-bef8-55d73e3ba2c5"}],"default_lakehouse_name":"DzungLH","default_lakehouse_workspace_id":"289e5f60-c84b-4572-be2d-8ea2d21262a2"}}},"cells":[{"cell_type":"code","source":["table_name = 'sales'"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3a6a53fb-3231-42ff-a798-d63701b8189e","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-14T13:17:50.535704Z","session_start_time":null,"execution_start_time":"2023-06-14T13:17:50.9595383Z","execution_finish_time":"2023-06-14T13:17:51.4132128Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"f03c00c0-7922-4d08-ba17-adc143f4b490"},"text/plain":"StatementMeta(, 3a6a53fb-3231-42ff-a798-d63701b8189e, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"tags":["parameters"]}},{"cell_type":"code","source":["from pyspark.sql.functions import *\r\n","\r\n","#read new sale data\r\n","df = spark.read.format('csv').option('header', 'true').load('Files/new_data/sales.csv')\r\n","\r\n","#add columns yeand and month\r\n","df = df.withColumn('Year', year(col('OrderDate'))).withColumn('Month', month(col('OrderDate')))\r\n","\r\n","#derive columns firstname & lastname\r\n","df = df.withColumn('FirstName', split(col('CustomerName'), ' ').getItem(0)).withColumn('LastName', split(col('CustomerName'), ' ').getItem(1))\r\n","\r\n","#select and reorder columns\r\n","df = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\r\n","\r\n","#save data to table\r\n","df.write.format('delta').mode('append').saveAsTable(table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"3a6a53fb-3231-42ff-a798-d63701b8189e","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-14T13:19:48.4381471Z","session_start_time":null,"execution_start_time":"2023-06-14T13:19:48.8944719Z","execution_finish_time":"2023-06-14T13:19:54.6387383Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":5,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4453,"rowCount":50,"jobId":16,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.functions import *\n\n#read new sale data\ndf = spark.read.format('csv').option('header', 'true').load('Files/new_data/sales.csv')\n\n#add columns yeand and month\ndf = df.withColumn('Year', year(col('OrderDate'))).withColumn('Month', month(col('OrderDate')))\n\n#derive columns firstname & lastname\ndf = df.withColumn('FirstName', split(col('CustomerName'), ' ').getItem(0)).withColumn('LastName', split(col('CustomerName'), ' ').getItem(1))\n\n#select and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n#save data to table\ndf.write.format('delta').mode('append').saveAsTable(table_name): Compute snapshot for version: 0","submissionTime":"2023-06-14T13:19:52.990GMT","completionTime":"2023-06-14T13:19:53.047GMT","stageIds":[24,25,23],"jobGroup":"7","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4453,"dataRead":2004,"rowCount":54,"jobId":15,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.functions import *\n\n#read new sale data\ndf = spark.read.format('csv').option('header', 'true').load('Files/new_data/sales.csv')\n\n#add columns yeand and month\ndf = df.withColumn('Year', year(col('OrderDate'))).withColumn('Month', month(col('OrderDate')))\n\n#derive columns firstname & lastname\ndf = df.withColumn('FirstName', split(col('CustomerName'), ' ').getItem(0)).withColumn('LastName', split(col('CustomerName'), ' ').getItem(1))\n\n#select and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n#save data to table\ndf.write.format('delta').mode('append').saveAsTable(table_name): Compute snapshot for version: 0","submissionTime":"2023-06-14T13:19:52.230GMT","completionTime":"2023-06-14T13:19:52.965GMT","stageIds":[21,22],"jobGroup":"7","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2004,"dataRead":2803,"rowCount":8,"jobId":14,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.functions import *\n\n#read new sale data\ndf = spark.read.format('csv').option('header', 'true').load('Files/new_data/sales.csv')\n\n#add columns yeand and month\ndf = df.withColumn('Year', year(col('OrderDate'))).withColumn('Month', month(col('OrderDate')))\n\n#derive columns firstname & lastname\ndf = df.withColumn('FirstName', split(col('CustomerName'), ' ').getItem(0)).withColumn('LastName', split(col('CustomerName'), ' ').getItem(1))\n\n#select and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n#save data to table\ndf.write.format('delta').mode('append').saveAsTable(table_name): Compute snapshot for version: 0","submissionTime":"2023-06-14T13:19:51.933GMT","completionTime":"2023-06-14T13:19:52.036GMT","stageIds":[20],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":456385,"dataRead":3311550,"rowCount":65436,"jobId":13,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 7:\nfrom pyspark.sql.functions import *\n\n#read new sale data\ndf = spark.read.format('csv').option('header', 'true').load('Files/new_data/sales.csv')\n\n#add columns yeand and month\ndf = df.withColumn('Year', year(col('OrderDate'))).withColumn('Month', month(col('OrderDate')))\n\n#derive columns firstname & lastname\ndf = df.withColumn('FirstName', split(col('CustomerName'), ' ').getItem(0)).withColumn('LastName', split(col('CustomerName'), ' ').getItem(1))\n\n#select and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n#save data to table\ndf.write.format('delta').mode('append').saveAsTable(table_name)","submissionTime":"2023-06-14T13:19:50.243GMT","completionTime":"2023-06-14T13:19:51.298GMT","stageIds":[19],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":12,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nfrom pyspark.sql.functions import *\n\n#read new sale data\ndf = spark.read.format('csv').option('header', 'true').load('Files/new_data/sales.csv')\n\n#add columns yeand and month\ndf = df.withColumn('Year', year(col('OrderDate'))).withColumn('Month', month(col('OrderDate')))\n\n#derive columns firstname & lastname\ndf = df.withColumn('FirstName', split(col('CustomerName'), ' ').getItem(0)).withColumn('LastName', split(col('CustomerName'), ' ').getItem(1))\n\n#select and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n#save data to table\ndf.write.format('delta').mode('append').saveAsTable(table_name)","submissionTime":"2023-06-14T13:19:49.080GMT","completionTime":"2023-06-14T13:19:49.195GMT","stageIds":[18],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"a7aa2c26-ab90-4f7b-b4d1-2148c6cd8477"},"text/plain":"StatementMeta(, 3a6a53fb-3231-42ff-a798-d63701b8189e, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}}]}